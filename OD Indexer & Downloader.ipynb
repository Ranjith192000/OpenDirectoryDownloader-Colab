{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Current Project - Downloader ODindexer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Fc0_OnVHWjRS",
        "TNL5ALxMArDT",
        "NDbYBgifZwHQ",
        "SUrqs9SDcnHS",
        "FSW5Tk0FZ3RK",
        "I-22r39zaKub",
        "uxGsFVl93F15",
        "xikAy3WDsGe8",
        "gBOshGNKTQjk",
        "OYoUwETyC-vW",
        "ZrrJPeTk7L7D"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2BEX3DsTj54"
      },
      "source": [
        "# Thanks to \n",
        "\n",
        "> KoalaBear84(https://github.com/KoalaBear84, https://www.reddit.com/user/KoalaBear84/)\n",
        "for creating OpenDirectoryDownloader\n",
        "\n",
        "> Jasimtanur (https://www.reddit.com/user/jasimtanur/)\n",
        "for creating code for both Installing and Indexing section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAhbE07yaglv"
      },
      "source": [
        "# To Up-Date\n",
        "\n",
        "\n",
        "  >Thumbnail Generator\n",
        "\n",
        "  >Gif Generator\n",
        "\n",
        "  >Better Filter Function\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc0_OnVHWjRS"
      },
      "source": [
        "# Read Before Using this Notebook\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Google Drive Indexing and Downloading Support Not Implemented Yet..**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Notebook Specification** : \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Disk Storage : 107 GB (Free 77GB)\n",
        "\n",
        "> RAM Memory   : 12 GB\n",
        "\n",
        "> Life Time    : 12 Hours (For Pro users 24 Hours)\n",
        "\n",
        "> Click on \"connect\" to connect to Virtual Machine.It can be done by many ways also\n",
        "\n",
        "> Left side File manager show what is inside \"/content/\" Folder\n",
        "\n",
        "For Google Drive Free Users:\n",
        "\n",
        "---\n",
        "\n",
        "> Maximum Drive space is 15GB, So only OD less than 14GB can be saved to Drive\n",
        "\n",
        "> To Download necessary files filter the links in URL file \n",
        "\n",
        "> Copy URL file to Google drive so that you can resume Downloading OD .Each time you run this notebook read Links from URL file located inside google drive\n",
        "\n",
        "\n",
        "For Google Drive Premium Users (100GB, 200GB, Unlimited) :\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> You can download most of the OD (based on size of these ODs)\n",
        "\n",
        "\n",
        "## Why I Created Colab Notebook\n",
        "\n",
        "> I created this notebook because the place I currenly living have issues with frequent power outage, poor tower signal (Very slow internet 1MB/Sec).\n",
        "\n",
        "> I cannot index or download due to this issues.\n",
        "\n",
        "> Thats is why I created a Colab notebook to automate downloading\n",
        "\n",
        "> With this notebook I can index OD and filter URL files, So that only necessary files will be archived and will be saved to Google Drive\n",
        "\n",
        "> I can download archives from Google Drive quickly and with pause and resume support\n",
        "\n",
        "\n",
        "\n",
        "## Usage :\n",
        "\n",
        "> Run each cell one by one by order.\n",
        "\n",
        "> Run only one instance of OD at a time.\n",
        "\n",
        "> If you want to run 2 or more OD then create copy of this notebook and run each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdmTnX9tgO-k"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS0NPK4uTF_X"
      },
      "source": [
        "Usage :\n",
        "\n",
        "        0.Run below cell \n",
        "        1.Click on the link\n",
        "        2.Select which Google drive account to store\n",
        "        3.Click on \"Allow\"\n",
        "        4.Copy the code\n",
        "        5.Paste the code in the rectangular box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sevckt82dAVq"
      },
      "source": [
        "# Mounting Google Drive to this Virtual Machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0T19NBdgSHH"
      },
      "source": [
        "# Installing dotnet and ODD-KB (Credit : Jasimtanur)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8mz0hQ5nlH",
        "cellView": "both"
      },
      "source": [
        "%%capture\n",
        "# Package to enable file management\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/OpenDirectoryDownloader-linux-x64.zip\"):\n",
        "  # Installing Microsoft product\n",
        "  !wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\n",
        "  !sudo dpkg -i packages-microsoft-prod.deb\n",
        "\n",
        "  # Updating Linux Environment\n",
        "  !sudo apt-get update; \\\n",
        "    apt-get install -y apt-transport-https && \\\n",
        "    apt-get update && \\\n",
        "    apt-get install -y dotnet-sdk-3.1\n",
        "  !sudo apt-get update; \\\n",
        "    sudo apt-get install -y apt-transport-https && \\\n",
        "    sudo apt-get update && \\\n",
        "    sudo apt-get install -y aspnetcore-runtime-3.1 \n",
        "\n",
        "  # Installing Dotnet\n",
        "  !sudo apt-get install -y dotnet-runtime-3.1\n",
        "\n",
        "  # Downloading OpenDirectoryDownloader-KoalaBear84 (ODD-KB): Version 1.6.0.2\n",
        "  !wget https://github.com/KoalaBear84/OpenDirectoryDownloader/releases/download/v1.6.0.2/OpenDirectoryDownloader-linux-x64.zip\n",
        "\n",
        "  # Extracting ODD-KB\n",
        "  !unzip /content/OpenDirectoryDownloader-linux-x64.zip -d /content/Indexer/\n",
        "else:\n",
        "  print('Necessary programs are installed already')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVU5BkKga2G"
      },
      "source": [
        "# Indexing (KoalaBear84 & Jasimtanur)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfcrr78VJ3Vi"
      },
      "source": [
        "#@markdown <h2>Lets Index the Open Directory</h2>\n",
        "\n",
        "OD_URL = \"www\" #@param {type:\"string\"}\n",
        "threads = \"20\" #@param {type:\"string\"}\n",
        "OD_title = \"www\" #@param {type:\"string\"}\n",
        "\n",
        "if OD_URL != '' and threads != '' and OD_title != '' :\n",
        "  arguments = '-u {} -t {} -q -j -s | tee \"{}.log\"'.format(OD_URL, threads, OD_title)\n",
        "  !dotnet /content/Indexer/OpenDirectoryDownloader.dll $arguments\n",
        "else:\n",
        "  print('Enter all values')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxnLfOLkLOeF"
      },
      "source": [
        "# Displaying Result Markdown\n",
        "with open('{}.log'.format(OD_title), 'r')as f1:\n",
        "  output = f1.read()\n",
        "\n",
        "if 'URLs with errors:' in output:\n",
        "  output = '\\n'.join(output[ output.rfind('|**Url:**'):output.rfind('URLs with errors:')-2].splitlines())\n",
        "else:\n",
        "  output = '\\n'.join(output[ output.rfind('|**Url:**'):output.rfind('Save session to JSON')-2].splitlines())\n",
        "\n",
        "from IPython.display import display, Markdown as md\n",
        "display(md(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nht9pcI_iCyA"
      },
      "source": [
        "# Threaded Download Initiator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnqXgzyiGSr"
      },
      "source": [
        "# Importing Necessary Packages\n",
        "from multiprocessing.dummy import Pool\n",
        "from multiprocessing import Pool as Pool_mp\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime as dati\n",
        "from pytz import timezone\n",
        "import os\n",
        "import json\n",
        "from google.colab import output as cell_output\n",
        "from threading import Thread\n",
        "import time\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "from urllib.parse import quote, unquote\n",
        "\n",
        "# Function to return file size\n",
        "def file_size(i):\n",
        "  n, p, d = 0, 1024, ['Bytes', 'KB', 'MB', 'GB', 'TB']\n",
        "  while i > p:\n",
        "    i /= p\n",
        "    n += 1\n",
        "  return '{:.2f} {}'.format(i, d[n])\n",
        "\n",
        "# Function to calculate a folder size\n",
        "def get_folder_size(folder):\n",
        "  total_size = 0\n",
        "  all_files = []\n",
        "  for path, dirs, files in os.walk(folder):\n",
        "    for file in files:\n",
        "      fp = os.path.join(path, file)\n",
        "      all_files.append(fp)\n",
        "      total_size += os.path.getsize(fp)\n",
        "  # print('Folder Name : {}\\nFolder size : {}\\nNumber of files : {}'.format(folder, file_size(total_size), len(all_files)))\n",
        "  return [folder, total_size, len(all_files)]\n",
        "\n",
        "# Function to calculate a folder size\n",
        "def show_folder_size(folder):\n",
        "  i = get_folder_size(folder)\n",
        "  print('Folder Name : {}\\nFolder size : {}\\nNumber of files : {}'.format(i[0], file_size(i[1]), i[2]))\n",
        "\n",
        "# Function to download files with Wget\n",
        "# def threaded_download_initiator_wget(i):\n",
        "#   !wget \"$i\" -nc -bqx -P \"Temp\" >/dev/null 2>&1\n",
        "#   tqdm_bar.update(1)\n",
        "#   return 0\n",
        "\n",
        "# Function to download files with Wget\n",
        "def threaded_download_initiator(i):\n",
        "  j = 0\n",
        "  size, url = i[0], i[1]\n",
        "  !wget \"$url\" -nc -x -P \"Temp\" >/dev/null 2>&1\n",
        "  tqdm_bar.update(1)\n",
        "  # print(j)\n",
        "  # j += 1\n",
        "  downloaded_size[0] += size\n",
        "  # print(j)\n",
        "  # j += 1\n",
        "  tqdm_bar_size.set_description_str('{:.1f}/{} MB: '.format(downloaded_size[0]/mb_size[0], total_mb_size[0]))\n",
        "  # print(j)\n",
        "  # j += 1\n",
        "  tqdm_bar_size.update(size)\n",
        "  # print(j)\n",
        "  # j += 1\n",
        "  return 0\n",
        "\n",
        "# Function to start thread\n",
        "def start_thread(target, number_of_threads):\n",
        "  # Creating number of threads \n",
        "  pool1 = Pool(number_of_threads)\n",
        "  # Running created threads\n",
        "  _ = pool1.map_async(threaded_download_initiator,target)\n",
        "  # Closing threads\n",
        "  pool1.close()\n",
        "  pool1.join()\n",
        "\n",
        "# Find all file types in json with it's size and count\n",
        "def find_filetypes(json_data):\n",
        "  popular_file_types = {}\n",
        "  for i in json_data:\n",
        "    temp_ext = os.path.splitext(i[1])[1].lower()\n",
        "    temp_size = i[0]\n",
        "\n",
        "    if temp_ext not in popular_file_types:\n",
        "      popular_file_types[temp_ext] = {'Count' : 1, 'Size' : temp_size}\n",
        "    else:\n",
        "      popular_file_types[temp_ext]['Count'] += 1\n",
        "      popular_file_types[temp_ext]['Size'] += temp_size\n",
        "  return popular_file_types\n",
        "\n",
        "# Function to show files types info\n",
        "def show_filestypes(file_types):\n",
        "  file_types = sorted([[i].extend(file_types[i]) for i in file_types], key=lambda x: x[2], reverse=True)\n",
        "\n",
        "  for i in file_types:\n",
        "    output_lines.append('{:6s} - {:10s} - {}'.format(i[0], str(i[0]), file_size(i[0])))\n",
        "  print('\\n'.join(output_lines))\n",
        "\n",
        "# Function to get file's size and URL\n",
        "def get_files_from_json(data):\n",
        "  temp = []\n",
        "  for i in data['Files']:\n",
        "    temp.append([i['FileSize'], i['Url']])\n",
        "  for i in data['Subdirectories']:\n",
        "    temp.extend(get_files_from_json(i))\n",
        "  return temp\n",
        "\n",
        "# Function to show head and tail of json data\n",
        "def show_head_tail(data, first = 10, last  = 10):\n",
        "  print('Number of Files in JSON : {}\\nFirst {} big files : \\n\\t{}'.format(len(data), first, '\\n\\t'.join(['{:15s} - {}'.format(file_size(i[0]), i[1]) for i in all_files_in_json[:first+1]])))\n",
        "  print('\\nLast {} files : \\n\\t{}'.format(last, '\\n\\t'.join(['{:15s} - {}'.format(file_size(i[0]), i[1]) for i in data[-last:]])))\n",
        "\n",
        "# Function to check whether given two file's sizes are nearly equal\n",
        "def is_close(a, b, n=10000):\n",
        "  if a == 0:\n",
        "    return True\n",
        "  t = (abs(a - b)/a) * 100\n",
        "  if t < 5:\n",
        "    return True\n",
        "  else:\n",
        "    print('Difference : ', )\n",
        "    return False\n",
        "\n",
        "# Function to check whether a file completely downloaded or not\n",
        "def check_progress(i):\n",
        "  file_size_1 = i[0]\n",
        "  file_path_1 = i[1]\n",
        "  percent_encoding_list = [ ['%20', ' '], ['%21', '!'], ['%22', '\"'], ['%23', '#'],\n",
        "                            ['%24', '$'], ['%25', '%'], ['%26', '&'], ['%27', \"'\"],\n",
        "                            ['%28', '('], ['%29', ')'], ['%2A', '*'], ['%2B', '+'],\n",
        "                            ['%2C', ','], ['%2D', '-'], ['%2E', '.'], ['%2F', '/'],\n",
        "                            ['%3A', ':'], ['%3B', ';'], ['%3C', '<'], ['%3D', '='],\n",
        "                            ['%3E', '>'], ['%3F', '?'], ['%40', '@'], ['%5B', '['],\n",
        "                           ['%5C', '\\\\'], ['%5D', ']'], ['%5E', '^'], ['%5F', '_'],\n",
        "                            ['%60', '`'], ['%7B', '{'], ['%7C', '|'], ['%7D', '}'],\n",
        "                            ['%7E', '~'], ['%C2%A3', '£']\n",
        "                           ]\n",
        "\n",
        "  t = i[1]\n",
        "  for i in ['http://', 'https://']:\n",
        "    t = t.replace(i, '')\n",
        "\n",
        "  for i in percent_encoding_list:\n",
        "    t = t.replace(i[0], i[1])\n",
        "  \n",
        "  file_path_2 = 'Temp/'+t\n",
        "\n",
        "  if not os.path.exists(file_path_2):\n",
        "    return 'Not Found', []\n",
        "  file_size_2 = os.path.getsize(file_path_2)\n",
        "  if is_close(file_size_1, file_size_2):\n",
        "    return True, [file_size_1, file_path_2, file_path_1]\n",
        "  else:\n",
        "    return False, [file_path_1, file_size_1, file_path_2, file_size_2, '{:05.2f}%'.format(100*(file_size_2/file_size_1))]\n",
        "\n",
        "# Function to check downloaded progress of each files\n",
        "def show_progress():\n",
        "  not_finished = []\n",
        "  finished_downloads = []\n",
        "  not_found = []\n",
        "  not_found_size = 0\n",
        "  for i in all_files_in_json:\n",
        "    x, y = check_progress(i)\n",
        "    if x == True:\n",
        "      finished_downloads.append(y)\n",
        "    elif x== False :\n",
        "      not_finished.append(y)\n",
        "    elif x == 'Not Found':\n",
        "      print('404 - Not Found : ', i)\n",
        "      not_found.append([i[0], i[1]])\n",
        "      not_found_size += i[0]\n",
        "  output_lines = []\n",
        "  error_size = 0\n",
        "  for i in not_finished:\n",
        "    error_size += i[1]\n",
        "    output_lines.append('{} - {:>10s}/{:10s} - {:80s} - {}'.format(i[4], file_size(i[3]), file_size(i[1]), i[2], i[0]))\n",
        "  output_lines.sort(reverse=True)\n",
        "  output_lines = ['{:03d} - {}'.format(i+1, j) for i, j in enumerate(output_lines)]\n",
        "  print('\\n'.join(output_lines))\n",
        "  print('Not finished ({}) : {} \\n'.format(len(not_finished), file_size(error_size)))\n",
        "  print('404 Not Found ({}) : {}'.format(len(not_found), file_size(not_found_size)))\n",
        "  return finished_downloads\n",
        "\n",
        "# Function to show biggest n files that are downloaded\n",
        "def show_biggest_in_finished(finished_downloads, n):\n",
        "  output_lines = sorted(finished_downloads[:n+1], key= lambda x: x[0], reverse=True)\n",
        "  output_lines = ['{:02d} - {:10s} - {:80s} - {}'.format(i+1, file_size(j[0]), j[1], j[2]) for i, j in enumerate(output_lines)]\n",
        "  print('Showing biggest files in finished :\\n{}'.format('\\n'.join(output_lines)))\n",
        "\n",
        "# Function to delete downloaded files\n",
        "def delete_downloaded_files(created_archives_list):\n",
        "  confirm = input('Please enter \"Y\" to delete all downloaded files, press other key to Cancel : \\n')\n",
        "  if confirm.lower() == 'y':\n",
        "    print('Deleted Temp Folder')\n",
        "    !rm -rf \"Temp\" >/dev/null 2>&1\n",
        "    !rm -rf \"test_thumb\" >/dev/null 2>&1\n",
        "    print('Deleted test_thumb Folder')\n",
        "    !rm -rf \"/content/Indexer/Scans/\" >/dev/null 2>&1\n",
        "    print('Deleted Scans Folder')\n",
        "\n",
        "    !rm \"running.log\" >/dev/null 2>&1\n",
        "\n",
        "    !rm \"Removed.txt\" >/dev/null 2>&1\n",
        "    !rm \"To Download.txt\" >/dev/null 2>&1\n",
        "\n",
        "    print('Deleted running.log')\n",
        "    !rm \"$OD_title\".log\n",
        "    print('Deleted {}.log'.format(OD_title))\n",
        "    \n",
        "    for i in created_archives_list:\n",
        "      !rm \"$i\"\n",
        "      print('Deleted ', i)\n",
        "    \n",
        "    !rm -rf \"/content/Arc\"\n",
        "    print('Deleted Arc Folder')\n",
        "    print('All Downloaded files are deleted :)')\n",
        "  else:\n",
        "    print('Deletion of Downloaded files has been Cancelled :)')\n",
        "\n",
        "# Function to show files types info\n",
        "def show_filestypes(file_types):\n",
        "  file_types = [[i, file_types[i]['Count'], file_types[i]['Size']] for i in file_types]\n",
        "  # print(file_types)\n",
        "  file_types = sorted(file_types, key=lambda x: x[2], reverse=True)\n",
        "  \n",
        "  output_lines = []\n",
        "  for i in file_types:\n",
        "    output_lines.append('{:6s} - {:10s} - {}'.format(i[0], str(i[1]), file_size(i[2])))\n",
        "  print('\\n'.join(output_lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RJ2lG1p-OiH"
      },
      "source": [
        "# Reading URL file and loading JSON data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNGSEH45GOr"
      },
      "source": [
        "# Getting file name of URL file\n",
        "temp = OD_URL\n",
        "for i in ['/', '?']:\n",
        "  temp = temp.replace(i, '_')\n",
        "for i in os.listdir('/content/Indexer/Scans/'):\n",
        "  if '.txt' in i:\n",
        "    i_file    = '/content/Indexer/Scans/{}'.format(i)\n",
        "    json_file = i_file.replace('.txt', '.json').replace(' - Filtered', '')\n",
        "\n",
        "# Loading JSON\n",
        "# json_file = '/content/1.json'\n",
        "print('Loading Json File : {}'.format(json_file))\n",
        "with open(json_file, 'r')as f1:\n",
        "  json_data = json.load(f1)\n",
        "\n",
        "# i_file = \"1.txt\"\n",
        "print('Opening URL File : {}'.format(i_file))\n",
        "if os.path.exists(i_file):\n",
        "  # Opening URL file in Read mode\n",
        "  with open(i_file, 'r')as f1:\n",
        "    source = sorted(f1.read().splitlines())\n",
        "  print('Number of record = {}'.format(len(source)))\n",
        "else:\n",
        "  print('URL File Not Found, Please Specify it manually')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKjbNm1EjyY7"
      },
      "source": [
        "all_files_in_json = sorted(get_files_from_json(json_data['Root']), key=lambda x: x[0], reverse=True)\n",
        "all_files_in_json_dict = {}\n",
        "for i in all_files_in_json:\n",
        "  all_files_in_json_dict[i[1]] = i[0]\n",
        "file_types = find_filetypes(all_files_in_json)\n",
        "show_filestypes(file_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjSzOOj7jlEd"
      },
      "source": [
        "### Filter URL if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6XrWCgdQzi_"
      },
      "source": [
        "#@title\n",
        "Filter_status = True #@param {type:\"boolean\"}\n",
        "skip_string = \"/thumbs/thumbs_\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "filtered_download_size = 0\n",
        "\n",
        "if Filter_status :\n",
        "  import re\n",
        "  # Filter Input URL File\n",
        "  o_file = i_file.replace('.txt', ' - Filtered.txt')\n",
        "  \n",
        "  # Use 0-skip or 1-accept\n",
        "  skip_or_accept = 1\n",
        "  if skip_or_accept == 0:\n",
        "    skip_exts = ['.mp4', '.mov']\n",
        "    filtered_list = sorted([i for i in source if re.search('-[0-9]+x[0-9]+', i) == None and [j for j in skip_exts if j in i.lower()] == [] and skip_string not in i])\n",
        "    removed_elements_in_list = list(set(source) - set(filtered_list))\n",
        "  elif skip_or_accept == 1:\n",
        "    accept_exts = ['.jpg', '.png', '.jpeg']\n",
        "    filtered_list = sorted([i for i in source if re.search('-[0-9]+x[0-9]+', i) == None and [j for j in accept_exts if j in i.lower()] != [] and skip_string not in i])\n",
        "    removed_elements_in_list = sorted(list(set(source) - set(filtered_list)))\n",
        "\n",
        "  print('Number of lines in input file : {}'.format(len(source)))\n",
        "  print('Number of lines in Output file : {}'.format(len(filtered_list)))\n",
        "  print('Number of lines in Removed : {}'.format(len(removed_elements_in_list)))\n",
        "\n",
        "  target = [[all_files_in_json_dict[i], i] for i in filtered_list]\n",
        "  target = [[i[0], quote(i[1]).replace('%3A//', '://')] for i in target]\n",
        "  with open(o_file, 'w')as f1:\n",
        "    f1.write('\\n'.join(filtered_list))\n",
        "\n",
        "  all_files_in_json = [i for i in all_files_in_json if i[1] not in removed_elements_in_list]\n",
        "  filtered_download_size = sum([i[0] for i in all_files_in_json])\n",
        "  if len(removed_elements_in_list) >11:\n",
        "    print('Number of removed : {}\\n{}\\n{}'.format(len(removed_elements_in_list), '\\n'.join(removed_elements_in_list[:11]), '\\n'.join(removed_elements_in_list[-11:])))\n",
        "  else:\n",
        "    print('Number of removed : {}\\n{}\\n'.format(len(removed_elements_in_list), '\\n'.join(removed_elements_in_list[:11])))\n",
        "  print('Avoided Unneccessary Download Size : {} and Files Count : {}'.format(file_size(json_data['TotalFileSizeEstimated'] - filtered_download_size), len(removed_elements_in_list)))\n",
        "  \n",
        "  with open('Removed.txt', 'w') as f1:\n",
        "    f1.write('\\n'.join(removed_elements_in_list))\n",
        "else:\n",
        "  filtered_download_size = json_data['TotalFileSizeEstimated']\n",
        "  target = [[all_files_in_json_dict[i], i] for i in source]\n",
        "  target = [[i[0], quote(i[1]).replace('%3A//', '://')] for i in target]\n",
        "\n",
        "with open('To Download.txt', 'w')as f1:\n",
        "  f1.write('\\n'.join([i[1] for i in target]))\n",
        "\n",
        "print('To Download Size : {}'.format(file_size(filtered_download_size)))\n",
        "show_head_tail(all_files_in_json, 10, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb5bV9kf-TPu"
      },
      "source": [
        "# Calling Downloader function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwDJ0Y3dkBgn"
      },
      "source": [
        "# Calling Download function\n",
        "mb_size = [2**20]\n",
        "total_mb_size = [int(filtered_download_size/mb_size[0])]\n",
        "downloaded_size = [0]\n",
        "tqdm_bar = tqdm(total=len(target), desc='Count', unit=' Count')\n",
        "tqdm_bar_size = tqdm(total=filtered_download_size, unit=' Bytes')\n",
        "start_thread(target, 10)\n",
        "tqdm_bar.close()\n",
        "tqdm_bar_size.close()\n",
        "# print('All Downloads are Started.\\nFiles with fast speed will be downloaded instanly.\\nSlow Speed Downloads tends to take time.\\nRun folder_size function to check progress of downloads')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UhLA440WR8I"
      },
      "source": [
        "    The Downloader will initiate wget for each file.\n",
        "\n",
        "    If a file size is larger than 1GB than it may took a while to get downloaded in the background.\n",
        "\n",
        "    Run below cell to calculate current downloaded size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXq8N_9TVHH"
      },
      "source": [
        "## Calculate Downloaded size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9d09Rw1WViS"
      },
      "source": [
        "#while True:\n",
        "# Calculating Folder Size\n",
        "i = get_folder_size('Temp')\n",
        "downloaded_size = i[1]\n",
        "total_download_size = filtered_download_size\n",
        "percent_downloaded = (downloaded_size/total_download_size) * 100\n",
        "print('\\r{:.2f} % Finished - ({}/{}) - {} to download'.format(percent_downloaded, file_size(downloaded_size), file_size(total_download_size), file_size(total_download_size-downloaded_size)), end='')\n",
        "if percent_downloaded > 95 :\n",
        "  print('\\nDownloading Finished')\n",
        "  #break\n",
        "#time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1QjBKV-wgX4"
      },
      "source": [
        "# total_files = json_data['TotalFiles']\n",
        "# total_size = json_data['TotalFileSizeEstimated']\n",
        "# speed_test = '{} per Second'.format(file_size(int(json_data['SpeedtestResult']['DownloadedBytes']/(json_data['SpeedtestResult']['ElapsedMilliseconds']/1000))))\n",
        "\n",
        "finished_downloads = show_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqjmVHDS7oXl"
      },
      "source": [
        "show_biggest_in_finished(finished_downloads, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T0g3RNZK2GJ"
      },
      "source": [
        "Currently Running Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQt2q9RwK46x"
      },
      "source": [
        "!ps aux | tee running.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzicSHXQQQh"
      },
      "source": [
        "with open('running.log', 'r')as f1:\n",
        "  wget_processes = len([i for i in f1.read().splitlines() if 'wget' in i])\n",
        "print('{} instances of Wget are running'.format(wget_processes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5NKRYG8CkZp"
      },
      "source": [
        "show_folder_size('Temp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7_LSveifpZ4"
      },
      "source": [
        "# Archiver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpBGAwU5r4xn"
      },
      "source": [
        "# Copy Scans folder to Temp folder\n",
        "index_files_prefix = unquote(OD_URL.replace('/', '_'))\n",
        "index_files = ['/content/Indexer/Scans/'+i for i in os.listdir('/content/Indexer/Scans/') if index_files_prefix in i]\n",
        "scan_path = '/content/Temp/Scans'\n",
        "if not os.path.exists(scan_path):\n",
        "  os.makedirs(scan_path)\n",
        "for i in index_files:\n",
        "  !rsync -ah --progress \"$i\" \"$scan_path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI49oLl-cR-X"
      },
      "source": [
        "# Creating name for Archive\n",
        "i_folder = \"Temp\"\n",
        "o_arc_file = OD_title\n",
        "# o_arc_file = \"PB\"\n",
        "archive_folder = 'Arc'\n",
        "if not os.path.exists(archive_folder):\n",
        "  os.makedirs(archive_folder)\n",
        "o_arc_file = '{}/{} - {}.7z'.format(archive_folder, dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S'), o_arc_file)\n",
        "o_arc_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It67typNl6EB"
      },
      "source": [
        "## Compressing folder with 7Zip\n",
        "# a    - Create Archive\n",
        "# m0   - Specify Compression\n",
        "# Copy - No Compression\n",
        "# v    - Split archive with size specified\n",
        "# 1g   - Split archive with each 1GB\n",
        "\n",
        "i = get_folder_size('Temp')\n",
        "downloaded_size = i[1]\n",
        "one_gb_size = 2**30\n",
        "\n",
        "# import getpass\n",
        "# star_pas = getpass.getpass()\n",
        "star_pas = \"1234567890\"\n",
        "!7z a -m0=Copy \"$o_arc_file\" \"$i_folder\" -v500m -mhe -p\"$star_pas\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7aY5NgK68TM"
      },
      "source": [
        "# Test archived files\n",
        "!7z t \"$o_arc_file\".001 -p$star_pas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERegbht6fswZ"
      },
      "source": [
        "# Copier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB6VuC5sxeMB"
      },
      "source": [
        "# Copy created archive to Google Drive\n",
        "created_archives_list = []\n",
        "copy_to = \"/content/drive/My Drive/My Files/Temp\"\n",
        "\n",
        "if not os.path.exists(copy_to):\n",
        "  os.makedirs(copy_to)\n",
        "\n",
        "copying_time = time.time()\n",
        "# List files in current directory\n",
        "for i in sorted(os.listdir(archive_folder)):\n",
        "  # Check if current file is a archive\n",
        "  if os.path.split(o_arc_file)[1] in i :\n",
        "    created_archives_list.append(os.path.join(archive_folder, i))\n",
        "  elif \"Thumbnails.7z\" in i :\n",
        "    created_archives_list.append(os.path.join(archive_folder, i))\n",
        "\n",
        "print(len(created_archives_list))\n",
        "created_archives_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFGreobJ6z8A"
      },
      "source": [
        "# Copying each files to Destination\n",
        "copy_archive_list = created_archives_list\n",
        "copy_archive_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHXr54URhb4"
      },
      "source": [
        "for i, j in enumerate(copy_archive_list):\n",
        "  print(j)\n",
        "  print('\\n\\n{}/{} : ({}) {}'.format(i+1, len(copy_archive_list), file_size(os.path.getsize(j)), j))\n",
        "  temp_cp_time = time.time()\n",
        "  !rsync -ah --progress \"$j\" \"$copy_to\"\n",
        "  temp_cp_time = time.time() - temp_cp_time\n",
        "  print('Time Took to Copy this file : {}'.format(time.strftime('%H-%M-%S', time.gmtime(temp_cp_time))))\n",
        "\n",
        "copying_time = time.time() - copying_time\n",
        "print('\\nTotal time took to copy files : {}'.format(time.strftime('%H-%M-%S', time.gmtime(copying_time))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpREorpL9TNT"
      },
      "source": [
        "show_folder_size(copy_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye-fjYwf6KUw"
      },
      "source": [
        "# Delete Downloaded files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWsuOYQB54fG"
      },
      "source": [
        "# Delete Downloaded files\n",
        "delete_downloaded_files(created_archives_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNL5ALxMArDT"
      },
      "source": [
        "# Gofile.io Uploading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIdMexdkBdQ7"
      },
      "source": [
        "If you want to use Gofile.io, Then change enabled to True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE2N_0oxBc30"
      },
      "source": [
        "enabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLG8QfPtCKOF"
      },
      "source": [
        "## Defining Functions for Gofile.io API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W71chr_DAs8R"
      },
      "source": [
        "def get_server():\n",
        "  print('Getting Fastest Gofile.io server.',end='')\n",
        "  server_checking_url = 'https://apiv2.gofile.io/getServer'\n",
        "  log_file = '1.log'\n",
        "  !curl $server_checking_url >$log_file 2>/dev/null\n",
        "  print('.', end='')\n",
        "  fast_server_info = !cat $log_file\n",
        "  fast_server_info = json.loads(fast_server_info[0])\n",
        "  if fast_server_info['status'] == 'error':\n",
        "    print('Cannot get Server URL')\n",
        "    return 'Error'\n",
        "\n",
        "  upload_url = 'https://{}.gofile.io/uploadFile'.format(fast_server_info['data']['server'])\n",
        "  print('.{}\\n'.format(upload_url))\n",
        "  return upload_url\n",
        "\n",
        "\n",
        "def upload_file(file_name, email):\n",
        "  upload_url = get_server()\n",
        "\n",
        "  if upload_url != 'Error':\n",
        "    if not os.path.exists(file_name):\n",
        "      print('File to upload not found : {}'.format(file_name))\n",
        "      return {}\n",
        "    else:\n",
        "      log_file = \"1.log\"\n",
        "      !curl -F email=$email -F file=@\"$file_name\" $upload_url >$log_file\n",
        "      response = !cat $log_file\n",
        "\n",
        "      if response == []:\n",
        "        print('No Response from server')\n",
        "        return {}\n",
        "      response = json.loads(response[0])\n",
        "      \n",
        "      if response['status'] == 'error':\n",
        "        print('Error uploading file')\n",
        "        return {}\n",
        "      response = response['data']\n",
        "      response['url_of_file'] = 'https://gofile.io/d/{}'.format(response['code'])\n",
        "      return response\n",
        "\n",
        "def show_upload_file_info(response):\n",
        "  if response != {}:\n",
        "    print('\\n\\nFile name   : {}'.format(response['file']['name']))\n",
        "    print('Admic code to file : {}'.format(response['adminCode']))\n",
        "    print('URL to file : {}\\n\\n'.format(response['url_of_file']))\n",
        "\n",
        "\n",
        "def acc_info(token):\n",
        "  print('Getting Account Info')\n",
        "  account_info_url = 'https://apiv2.gofile.io/getAccountInfo?token={}'.format(token)\n",
        "  log_file = \"1.log\"\n",
        "  !curl $account_info_url >$log_file 2>/dev/null\n",
        "  res = !cat $log_file\n",
        "  res = json.loads(res[0])\n",
        "  if res['status'] == 'error':\n",
        "    print('Invalid Token Given')\n",
        "    return {}\n",
        "  \n",
        "  res = res['data']\n",
        "  return res\n",
        "\n",
        "def show_acc_info(result):\n",
        "  if result != {}:\n",
        "    print('\\nAccount Info : \\n\\tEmail   : {}\\n\\tAccount : {}\\n\\tFiles Count : {}\\n\\tFiles Size  : {}'.format(result['email'], result['account'], result['filesCount'], file_size(result['filesSize'])))\n",
        "\n",
        "\n",
        "def user_uploads_info(token):\n",
        "  print('Getting Uploads Info')\n",
        "  files_info_url   = 'https://apiv2.gofile.io/getUploadsList?token={}'.format(token)\n",
        "  log_file = '1.log'\n",
        "  !curl $files_info_url >$log_file 2>/dev/null\n",
        "  res = !cat $log_file\n",
        "  r = json.loads(res[0])\n",
        "  if r['status'] == 'error':\n",
        "    print('Invalid Token Given')\n",
        "    return {}\n",
        "\n",
        "  r = r['data']\n",
        "  for index_i, i in enumerate(r):\n",
        "    r[i]['File_URL'] = 'https://gofile.io/d/{}'.format(r[i]['code'])\n",
        "  return r\n",
        "\n",
        "def show_user_uploads_info(r):\n",
        "  if r != {}:\n",
        "    uploads_count = len(r)\n",
        "    print('Uploads Info :\\nNumber of Uploads : {}'.format(uploads_count))\n",
        "\n",
        "    for index_i, i in enumerate(r):\n",
        "      link = r[i]['File_URL']\n",
        "      files_count = len(r[i]['files'])\n",
        "      temp_output = []\n",
        "\n",
        "      for index_j, j in enumerate(r[i]['files']):\n",
        "        name = r[i]['files'][j]['name']\n",
        "        size = file_size(r[i]['files'][j]['size'])\n",
        "        \n",
        "        try:\n",
        "          md5  = r[i]['files'][j]['md5']\n",
        "        except:\n",
        "          md5  = 'Unknown'\n",
        "        temp_output.append('({}/{}) : {}\\n\\t\\t\\tMD5  : {}\\n\\t\\t\\tSize : {}'.format(index_j+1, files_count, name, md5, size))\n",
        "\n",
        "      try:\n",
        "        views = r[i]['views']\n",
        "      except:\n",
        "        views = 'Unknown'\n",
        "      \n",
        "      downloads = r[i]['downloaded']\n",
        "      total_size = file_size(r[i]['totalSize'])\n",
        "      uploaded_time = dati.utcfromtimestamp(int(r[i]['uploadTime'])).strftime('%Y-%m-%d %H:%M:%S')\n",
        "      expire_time   = dati.utcfromtimestamp(int(r[i]['removalDate'])).strftime('%Y-%m-%d %H:%M:%S')\n",
        "      admin_code    = r[i]['adminCode']\n",
        "\n",
        "      t = '\\t({}/{}) : {}\\n\\t\\tFiles Count : {}\\n\\t\\tViews      : {}\\n\\t\\tDownloads  : {}\\n\\t\\tTotal Size : {}\\n\\t\\tUploaded Time : {}\\n\\t\\tExpiry Date   : {}\\n\\t\\tAdmin Code    : {}\\n\\t\\tFiles : \\n\\t\\t\\t{}'\n",
        "      print(t.format(index_i+1, uploads_count, link, files_count, views, downloads, total_size, uploaded_time, expire_time, admin_code, '\\n'.join(temp_output)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZSDnNqbBBLy"
      },
      "source": [
        "## Upload File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLKD0GBlA9_X"
      },
      "source": [
        "# Use your email address if you have account in Gofile.io\n",
        "file_name = '/content/sample_data/california_housing_train.csv'\n",
        "email = 'yourgmailid@gmail.com'\n",
        "\n",
        "if enabled == True:\n",
        "  result = upload_file(file_name, email)\n",
        "  show_upload_file_info(result)\n",
        "else:\n",
        "  print('Gofile.io Operations not enabled\\nTo enable this, Change value of variable named \"enabled\" to True ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfvSUM2QBDEZ"
      },
      "source": [
        "## Batch Upload Single Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3L9Vlsl3VNf"
      },
      "source": [
        "email = '123@gmail.com'\n",
        "files_to_upload = [i for i in os.listdir('.') if o_arc_file in i]\n",
        "files_to_upload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkr2fm09BEk4"
      },
      "source": [
        "# Upload Multiple Files\n",
        "uploads_details = []\n",
        "if enabled == True:\n",
        "  for i in files_to_upload:\n",
        "    result = upload_file(i, email)\n",
        "    show_upload_file_info(result)\n",
        "else:\n",
        "  print('Gofile.io Operations not enabled\\nTo enable this, Change value of variable named \"enabled\" to True ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbBgq2VnBLzp"
      },
      "source": [
        "## Get Account Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC1yuDOKBU9K"
      },
      "source": [
        "# Pass your Gofile.io Account's Token\n",
        "token = 'gdfgdfgdgf241ce713d13bfccbfd77c0362357c'\n",
        "\n",
        "if enabled == True:\n",
        "  result = acc_info(token)\n",
        "  show_acc_info(result)\n",
        "else:\n",
        "  print('Gofile.io Operations not enabled\\nTo enable this, Change value of variable named \"enabled\" to True ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsU754Z1BYjT"
      },
      "source": [
        "## Uploaded Files Info\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8I_p6KABba4"
      },
      "source": [
        "# Pass your Gofile.io Account's Token\n",
        "token = 'c8b22a44a8c7ab3a6241ce713d13bfccbdfgdfgdfgdfg357c'\n",
        "\n",
        "if enabled == True:\n",
        "  result = user_uploads_info(token)\n",
        "  show_user_uploads_info(result)\n",
        "else:\n",
        "  print('Gofile.io Operations not enabled\\nTo enable this, Change value of variable named \"enabled\" to True ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vw71UwQvBXS"
      },
      "source": [
        "# Testing Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDbYBgifZwHQ"
      },
      "source": [
        "## Video thumbnail Generator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf68vhq_tE96"
      },
      "source": [
        "# Download Aerial.ttf Font\n",
        "!wget \"https://drive.google.com/uc?id=1jZDDngJNKvnDC6vn6rzUobN0SDuifiDK&export=download\" -O \"Aerial.ttf\"\n",
        "# Refer : https://stackoverflow.com/questions/27568254/how-to-extract-1-screenshot-for-a-video-with-ffmpeg-at-a-given-time\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "from multiprocessing.dummy import Pool\n",
        "\n",
        "from datetime import datetime as dati\n",
        "from pytz import timezone\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import output as colab_output\n",
        "from IPython.display import Image as colab_image\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def ffmpeg_get_screenshot(i):\n",
        "  with open(os.devnull, 'wb') as devnull:\n",
        "    subprocess.check_call(['ffmpeg', '-ss', i[0], '-i', i[1], '-y', '-vframes', '1', i[2]], stdout=devnull, stderr=subprocess.STDOUT)\n",
        "\n",
        "def start_collage(list_of_images, label_list, video_thumb_folder, video_name):\n",
        "  n = len(list_of_images)\n",
        "  matrix_size = 7\n",
        "  matrix_total_size = matrix_size * matrix_size\n",
        "  number_of_collages = n / (matrix_total_size)\n",
        "  number_of_collages = int(number_of_collages)+1 if number_of_collages > int(number_of_collages) else number_of_collages\n",
        "  \n",
        "  black_webp = 'Temp_Blank.webp'\n",
        "  bl_img = Image.new('RGB', (1000, 1000))\n",
        "  bl_img.save(black_webp, \"WEBP\")\n",
        "\n",
        "  blank_img_to_add = (matrix_total_size*number_of_collages) - n\n",
        "  for i in range(blank_img_to_add):\n",
        "    list_of_images.append(black_webp)\n",
        "    label_list.append('')\n",
        "  \n",
        "  for i in range(number_of_collages):\n",
        "    output_collage = \"{}/_{} - {}.webp\".format(video_thumb_folder, video_name.replace('.', '_'), i+1)\n",
        "    create_collage(list_of_images[matrix_total_size*i:matrix_total_size*(i+1)], output_collage, img_type='WEBP', width=2000, height=2000, cols=7, rows=7, label_list=label_list[matrix_total_size*i:matrix_total_size*(i+1)])\n",
        "  os.remove(black_webp)\n",
        "\n",
        "def create_collage(listofimages, o_file, img_type='WEBP', width=2000, height=2000,cols=5,rows=4, label_list=[]):\n",
        "  if cols == 0:\n",
        "    return\n",
        "  max_cells = 100\n",
        "  thumbnail_width = width//cols\n",
        "  thumbnail_height = height//rows\n",
        "  size = thumbnail_width, thumbnail_height\n",
        "  new_im = Image.new('RGB', (width, height), (100, 100, 100))\n",
        "  \n",
        "  draw = ImageDraw.Draw(new_im)\n",
        "  font = ImageFont.truetype('Aerial.ttf', 20)\n",
        "  ims = []\n",
        "  text_coord = []\n",
        "  for p in listofimages:\n",
        "    try:\n",
        "      im = Image.open(p)\n",
        "    except:\n",
        "      im = Image.new('RGB', (500,500))\n",
        "    im.thumbnail(size)\n",
        "    ims.append(im)\n",
        "  i = x = y = 0\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      text_coord.append([x + thumbnail_width*.3, y + thumbnail_height*.7])\n",
        "      new_im.paste(ims[i], (x, y))      \n",
        "      i += 1\n",
        "      x += thumbnail_width\n",
        "    y += thumbnail_height\n",
        "    x = 0\n",
        "  for index_i, i in enumerate(text_coord):\n",
        "    draw.text((i[0], i[1]), label_list[index_i],(255,255,255),font=font)\n",
        "  new_im.save(o_file,img_type)\n",
        "\n",
        "def video_to_img(video_folder, video_name, video_thumb_folder, for_seconds=60):\n",
        "  video_path = \"{}/{}\".format(video_folder, video_name)\n",
        "  result = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', video_path], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "  video_duration = float(result.stdout)\n",
        "  no_of_imgs = int(video_duration/for_seconds)\n",
        "\n",
        "  if not os.path.exists(video_thumb_folder):\n",
        "    os.makedirs(video_thumb_folder)\n",
        "  \n",
        "  print('{:>5s} Seconds - {:>6s} Parts - Path : {}\\n'.format(str(int(video_duration)), str(no_of_imgs), video_path))\n",
        "  zeros_size = len(str(no_of_imgs))\n",
        "  list_of_images = []\n",
        "  label_list = []\n",
        "  ffmpeg_commands = []\n",
        "\n",
        "  for i in range(1, no_of_imgs+1 ):\n",
        "    time_pointer =  time.strftime('%H:%M:%S', time.gmtime(i*for_seconds))\n",
        "    label_list.append(time_pointer)\n",
        "    out_img_path = \"{}/{}.jpg\".format(video_thumb_folder, time_pointer.replace(':', '_'))\n",
        "    list_of_images.append(out_img_path)\n",
        "    ffmpeg_commands.append([time_pointer, video_path, out_img_path])\n",
        "  \n",
        "  with Pool(10) as p:\n",
        "    r = list(tqdm(p.imap(ffmpeg_get_screenshot, ffmpeg_commands), total=len(ffmpeg_commands)))\n",
        "  \n",
        "  start_collage(list_of_images, label_list, video_thumb_folder, video_name)\n",
        "  print('\\nPhoto Collage Created')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11HCcj3z2wBM"
      },
      "source": [
        "vi_in_temp = []\n",
        "for path, dirs, files in os.walk('Temp'):\n",
        "  for file in files:\n",
        "    if '.mp4' in file:\n",
        "      vi_in_temp.append([path, file])\n",
        "print('Number of Files : ', len(vi_in_temp))\n",
        "print('\\n'.join(str(i) for i in vi_in_temp[:20]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUF5oCARH6cM"
      },
      "source": [
        "tqdm_bar = tqdm(total=len(vi_in_temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwLcldB1TCH"
      },
      "source": [
        "vi_thumbnail_folder = '/content/Vi_Thumbnails'\n",
        "for i in vi_in_temp:\n",
        "  video_folder = i[0]\n",
        "  video_name = i[1]\n",
        "  video_thumb_folder = \"{}/{}-Thumbs\".formatvi_thumbnail_folder, (os.path.splitext(video_name)[0])\n",
        "  if not os.path.exists(video_thumb_folder):\n",
        "    video_to_img(video_folder, video_name, video_thumb_folder, 10)\n",
        "  tqdm_bar.update(1)\n",
        "  colab_output.clear()\n",
        "tqdm_bar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXo6bLqPJTsu"
      },
      "source": [
        "show_folder_size(get_folder_size(vi_thumbnail_folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVS2n3IPljnG"
      },
      "source": [
        "## Need To Create Collage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUrqs9SDcnHS"
      },
      "source": [
        "## GIF Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKYINC2LJjmL"
      },
      "source": [
        "from IPython.display import display\n",
        "from ipywidgets import Dropdown\n",
        "from PIL import Image\n",
        "\n",
        "def create_small_gif(temp_folder_1, out_gif, gif_size):\n",
        "  filenames = sorted(['{}/{}'.format(temp_folder_1, i) for i in os.listdir(temp_folder_1)])\n",
        "  img = Image.new('RGB', gif_size)\n",
        "  imgs = [Image.open(f) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg', '.gif' ]]\n",
        "  for i in imgs:\n",
        "    i.thumbnail(gif_size)\n",
        "  img.save(fp=out_gif, format='GIF', append_images=imgs, save_all=True, duration=200, loop=0)\n",
        "\n",
        "def folder_to_gifs(folder, out_folder, resolution):\n",
        "  if not os.path.exists(out_folder):\n",
        "    os.makedirs(out_folder)\n",
        "  temp_folder_gifs = ['{}/{}'.format(folder, i) for i in sorted(os.listdir(folder))]\n",
        "  print(len(temp_folder_gifs))\n",
        "  print(temp_folder_gifs[0])\n",
        "  \n",
        "\n",
        "  tqdm_bar = tqdm(total=len(temp_folder_gifs))\n",
        "  for i in temp_folder_gifs:\n",
        "    create_small_gif(i, \"{}/{}.gif\".format(out_folder, os.path.split(i)[-1]), resolution)\n",
        "    tqdm_bar.update(1)\n",
        "  tqdm_bar.close()\n",
        "  #############################################################################\n",
        "  ##########################################################################\n",
        "  ################################################################\n",
        "  ######################################################\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "  colab_output.clear()\n",
        "  display(dropdown)\n",
        "  \n",
        "  def dropdown_img_eventhandler(change):\n",
        "    colab_output.clear()\n",
        "    display(dropdown)\n",
        "    display(dropdown_img)\n",
        "\n",
        "    current_img = change.new\n",
        "    print(current_img)\n",
        "    current_img = current_img[current_img.find('/'):]\n",
        "    print(current_img)\n",
        "\n",
        "    im = Image.open(current_img)\n",
        "    im.thumbnail((100, 100))\n",
        "    im.save('temp.jpeg', 'JPEG')\n",
        "    display(colab_image('temp.jpeg'))\n",
        "\n",
        "  current_folder = change.new\n",
        "  current_folder = \"/content/Vi_Thumbnails/{}\".format(current_folder[current_folder.find('. ')+2:])\n",
        "  sorted_folders = [ '{}/{}'.format(current_folder, i) for i in sorted(os.listdir(current_folder))]\n",
        "  option_list = ['{}. ({}) {}'.format(index_i+1, file_size(os.path.getsize(i)), i) for index_i, i in enumerate(sorted_folders)]\n",
        "  dropdown_img = Dropdown(description=\"Imgs : \", options=option_list)\n",
        "  dropdown_img.observe(dropdown_img_eventhandler, names='value')\n",
        "  display(dropdown_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lws-yJDvgeUR"
      },
      "source": [
        "gif_in_folder = \"/content/Vi_Thumbnails\"\n",
        "gif_out_folder = \"Medium_gifs\"\n",
        "folder_to_gifs(gif_in_folder, gif_out_folder, (500, 500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkieWbhdN373"
      },
      "source": [
        "colab_image(open('image.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndkutwl9fn-i"
      },
      "source": [
        "out_archive_file = '{} - {}.7z'.format(dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S'), os.path.split(gif_out_folder)[-1])\n",
        "out_archive_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sovjD4Y9eiyt"
      },
      "source": [
        "!7z a -m0=Copy \"$out_archive_file\" \"$gif_out_folder\" -mhe -p1234567890 -v300m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM-JbRe6gQIq"
      },
      "source": [
        "!7z t \"$out_archive_file\".001 -p1234567890"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S-iGD7Kg5dL"
      },
      "source": [
        "archives_list = []\n",
        "for i in sorted(os.listdir()):\n",
        "  if out_archive_file in i:\n",
        "    archives_list.append(i)\n",
        "archives_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKLbtgL5gcys"
      },
      "source": [
        "for i in archives_list:\n",
        "  !rsync -ah --progress \"$i\" \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_b6sirxdh-"
      },
      "source": [
        "sorted_folders = sorted(os.listdir(\"/content/Vi_Thumbnails\"))\n",
        "option_list = ['{}. {}'.format(index_i+1, i) for index_i, i in enumerate(sorted_folders)]\n",
        "dropdown = Dropdown(description=\"Vi_Th Folders : \", options=option_list)\n",
        "dropdown.observe(dropdown_eventhandler, names='value')\n",
        "display(dropdown)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSW5Tk0FZ3RK"
      },
      "source": [
        "## Download GD Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1VnYfFbZTPH"
      },
      "source": [
        "!pip install googledrivedownloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLJnnr-SZWrW"
      },
      "source": [
        "drive_link = \"https://drive.google.com/u/0/uc?id=1234q4mopxvZPwSPN4_s5EqY8sq3M_H&export=download\"\n",
        "drive_link_id = \"1234q4mopxvZPwSPN4_s5EqY8sq3M_H\"\n",
        "download_file_name = \"Data/1.zip\" #Should Contain folder naem\n",
        "to_unzip = False\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id=drive_link_id, dest_path=download_file_name, unzip=to_unzip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH82AVcsPLgc"
      },
      "source": [
        "!rsync -ah --progress \"1.zip\" \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-22r39zaKub"
      },
      "source": [
        "## Compress Video with FFMPEG YIFY-Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vyOGXMuZl83"
      },
      "source": [
        "download_file_name = \"/content/Tom and Jerry Tales s01 ep09.avi\"\n",
        "converted_file_name = \"/content/Tom and Jerry Tales s01 ep09 - Converted.mkv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6iDW7-iT0D"
      },
      "source": [
        "!ffprobe \"$download_file_name\" -show_entries format=nb_streams -v 0 -of compact=p=0:nk=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xArEXMrZoj9"
      },
      "source": [
        "!ffmpeg -i \"$download_file_name\" -c:v libx264 -crf 27 -x264-params cabac=1:ref=5:analyse=0x133:me=umh:subme=9:chroma-me=1:deadzone-inter=21:deadzone-intra=11:b-adapt=2:rc-lookahead=60:vbv-maxrate=10000:vbv-bufsize=10000:qpmax=69:bframes=5:b-adapt=2:direct=auto:crf-max=51:weightp=2:merange=24:chroma-qp-offset=-1:sync-lookahead=2:psy-rd=1.00,0.15:trellis=2:min-keyint=23:partitions=all -c:a aac -ar 44100 -b:a 128k -map 0 \"$converted_file_name\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgMzdsAhs1Hq"
      },
      "source": [
        "from IPython.display import FileLink, FileLinks\n",
        "display(FileLink(\"/content/sample_data/california_housing_test.csv\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZFJ0-XQui69"
      },
      "source": [
        "FileLinks('/content/sample_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxGsFVl93F15"
      },
      "source": [
        "## Download and Compress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHoIYjF9Lke"
      },
      "source": [
        "url     = \"https://www.zip\"\n",
        "file_to = \"1.zip\"\n",
        "password = \"1234567890\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs_0zxi73Qy0"
      },
      "source": [
        "import os\n",
        "\n",
        "ext_folder = os.path.splitext(file_to)[0]\n",
        "print(ext_folder)\n",
        "arc_file   = os.path.splitext(file_to)[0] + '.7z'\n",
        "print(arc_file)\n",
        "\n",
        "!wget -c $url -O \"$file_to\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KlmPJVI-sL3"
      },
      "source": [
        "!7z x \"$file_to\" -O\"$ext_folder\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P_REbOv_ws5"
      },
      "source": [
        "!7z a -m0=lzma2 -mx=9 \"$arc_file\" \"$ext_folder\" -mhe -p\"$password\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnr-QWS0Fk3U"
      },
      "source": [
        "!7z t \"$arc_file\" -p\"$password\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkClDKGx39sS"
      },
      "source": [
        "# Mounting Google Drive to this Virtual Machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr63xNKgGDxN"
      },
      "source": [
        "!rsync -ah --progress \"$arc_file\" \"/content/drive/My Drive/$arc_file\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikAy3WDsGe8"
      },
      "source": [
        "## PDF Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_h1j9A_thtm"
      },
      "source": [
        "### Premilinaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1tNP78xtoMj"
      },
      "source": [
        "!pip install PyPDF2\n",
        "!pip install textract\n",
        "!pip install rake_nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf01c-zZsMnR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W79LsNCjtabe"
      },
      "source": [
        "### Reading Text\n",
        "> Converted PDF file to txt format for better pre-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HpELTo7sSc3"
      },
      "source": [
        "filename ='/content/Deep Learning 2020/pytorch-Deep-Learning-master/slides/04 - RNN.pdf'\n",
        "\n",
        "pdfFileObj = open(filename,'rb')               #open allows you to read the file\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)   #The pdfReader variable is a readable object that will be parsed\n",
        "num_pages = pdfReader.numPages                 #discerning the number of pages will allow us to parse through all the pages\n",
        "\n",
        "\n",
        "count = 0\n",
        "text = \"\"\n",
        "                                                            \n",
        "while count < num_pages:                       #The while loop will read each page\n",
        "  pageObj = pdfReader.getPage(count)\n",
        "  count +=1\n",
        "  text += pageObj.extractText()\n",
        "    \n",
        "#Below if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
        "\n",
        "if text != \"\":\n",
        "  text = text\n",
        "    \n",
        "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
        "\n",
        "else:\n",
        "  text = textract.process('http://bit.ly/epo_keyword_extraction_document', method='tesseract', language='eng')\n",
        "\n",
        "  # Now we have a text variable which contains all the text derived from our PDF file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9ZePAesVDA"
      },
      "source": [
        "text = text.encode('ascii','ignore').lower().decode(\"utf-8\")  #Lowercasing each word\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx0NLuQKtV46"
      },
      "source": [
        "### Extracting Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3m2ypOmsXHS"
      },
      "source": [
        "keywords = re.findall(r'[a-zA-Z]\\w+',text)\n",
        "len(keywords)                               #Total keywords in document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGSKxImsahe"
      },
      "source": [
        "df = pd.DataFrame(list(set(keywords)),columns=['keywords'])  #Dataframe with unique keywords to avoid repetition in rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgrtJZfBs69b"
      },
      "source": [
        "### Calculating Weightage\n",
        "> In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
        "\n",
        "\n",
        "> **TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
        "\n",
        "\n",
        "**TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
        "\n",
        "> **IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
        "\n",
        "**IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVvWFlT6sdQ3"
      },
      "source": [
        "def weightage(word,text,number_of_documents=1):\n",
        "  word_list = re.findall(word,text)\n",
        "  number_of_times_word_appeared =len(word_list)\n",
        "  tf = number_of_times_word_appeared/float(len(text))\n",
        "  idf = np.log((number_of_documents)/float(number_of_times_word_appeared))\n",
        "  tf_idf = tf*idf\n",
        "  return number_of_times_word_appeared,tf,idf ,tf_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPtt8alpsf3T"
      },
      "source": [
        "df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: weightage(x,text)[0])\n",
        "df['tf'] = df['keywords'].apply(lambda x: weightage(x,text)[1])\n",
        "df['idf'] = df['keywords'].apply(lambda x: weightage(x,text)[2])\n",
        "df['tf_idf'] = df['keywords'].apply(lambda x: weightage(x,text)[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A3ttfr5shIh"
      },
      "source": [
        "df = df.sort_values('tf_idf',ascending=True)\n",
        "df.to_csv('Keywords.csv')\n",
        "df.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd9aZAVEs0p8"
      },
      "source": [
        "### Second Method - Using Gensim library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wquewrTkskGD"
      },
      "source": [
        "from gensim.summarization import keywords\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwKCCjW8slvY"
      },
      "source": [
        "values = keywords(text=text,split='\\n',scores=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bdJkhAsnSB"
      },
      "source": [
        "data = pd.DataFrame(values,columns=['keyword','score'])\n",
        "data = data.sort_values('score',ascending=False)\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzAr0fBssw6x"
      },
      "source": [
        "### Third Approach - Using RAKE (Rapid Automatic Keyword Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ZSM_Y6spC1"
      },
      "source": [
        "from rake_nltk import Rake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmT84E_TsreK"
      },
      "source": [
        "r = Rake()\n",
        "r.extract_keywords_from_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPVVPOtOsspM"
      },
      "source": [
        "phrases = r.get_ranked_phrases_with_scores()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxOH_tICsuUA"
      },
      "source": [
        "table = pd.DataFrame(phrases,columns=['score','Phrase'])\n",
        "table = table.sort_values('score',ascending=False)\n",
        "table.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOshGNKTQjk"
      },
      "source": [
        "## IMG Thumbnail generator (Incomplete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-G6RlhuMEk7"
      },
      "source": [
        "def thumb_gen(i):\n",
        "  size = 128, 128\n",
        "  outfile = i[1]\n",
        "  infile = i[0]\n",
        "  n = 0\n",
        "  while True:\n",
        "    n += 1\n",
        "    if os.path.exists(outfile):\n",
        "      temp = os.path.splitext(i[1])\n",
        "      outfile = '{} - {}.{}'.format(temp[0], n, temp[1])\n",
        "    else:\n",
        "      break\n",
        "  print(infile, outfile)\n",
        "  if infile != outfile:\n",
        "    try:\n",
        "      im = Image.open(infile)\n",
        "      im.thumbnail(size)\n",
        "      im.save(outfile, \"JPEG\")\n",
        "    except IOError:\n",
        "      print('Error : ',i)\n",
        "\n",
        "def create_thumbnails():\n",
        "  to_thumb_list = []\n",
        "  thumbnails_ext = ['.jpg', '.jpeg', '.png']\n",
        "  src_to_thumb = '/content/Temp/'\n",
        "  thumb_out = '/content/test_thumb/'\n",
        "  if not os.path.exists(thumb_out):\n",
        "    os.makedirs(thumb_out)\n",
        "  for path, dirs, files in os.walk(src_to_thumb):\n",
        "    for file in files:\n",
        "      if os.path.splitext(file)[1] in thumbnails_ext:\n",
        "        test_thumb_folder = path.replace(src_to_thumb, thumb_out)\n",
        "        if not os.path.exists(test_thumb_folder):\n",
        "          os.makedirs(test_thumb_folder)\n",
        "        to_thumb_list.append([os.path.join(path, file), test_thumb_folder+'/'+file])\n",
        "  \n",
        "  print('First record : \\n', to_thumb_list[0])\n",
        "  input('Sample : ')\n",
        "  print('Number of Imgs to Thumbnail : {}'.format(len(to_thumb_list)))\n",
        "  thumb_out = 'test_thumb/'\n",
        "  !rm -rf \"$thumb_out\"\n",
        "  if not os.path.exists(thumb_out):\n",
        "    os.makedirs(thumb_out)\n",
        "  with Pool_mp(1) as p:\n",
        "    r = list(tqdm(p.imap(thumb_gen, to_thumb_list[:10]), total=len(to_thumb_list[:10])))\n",
        "  print('Number of thumbnails  : {}\\n\\n'.format(len(os.listdir('test_thumb'))))\n",
        "  o_arc_file = '{} - {} Thumbnails.7z'.format(dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S'), OD_title)\n",
        "  !7z a -m0=Copy \"$o_arc_file\" \"$thumb_out\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ecou7bmIynE"
      },
      "source": [
        "temp = \"/content/Temp/them/images\"\n",
        "a = '{}/{}'.format(temp, os.listdir(temp)[0])\n",
        "b = a.replace('/content/Temp', '/content/test_thumb')\n",
        "os.makedirs(os.path.split(temp)[0])\n",
        "# print(a, b)\n",
        "\n",
        "im = Image.open(a)\n",
        "im.thumbnail((128, 128))\n",
        "im.save(b, \"JPEG\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUy2rN_IUmD6"
      },
      "source": [
        "enabled_thumb = True\n",
        "if enabled_thumb == True:\n",
        "  create_thumbnails()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoUwETyC-vW"
      },
      "source": [
        "## Video To IMG "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbN2FFzJC-N2"
      },
      "source": [
        "# Install FFMPEG Python Package\n",
        "print('Installing FFMPEG', end='')\n",
        "!pip install ffmpeg-python >/dev/null 2>&1\n",
        "\n",
        "# Download Aerial.ttf Font\n",
        "!wget \"https://drive.google.com/uc?id=1jZDDngJNKvnDC6vn6rzUobN0SDuifiDK&export=download\" -O \"Aerial.ttf\" >/dev/null 2>&1\n",
        "\n",
        "# Refer : https://stackoverflow.com/questions/27568254/how-to-extract-1-screenshot-for-a-video-with-ffmpeg-at-a-given-time\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import ffmpeg\n",
        "from tqdm.notebook import tqdm\n",
        "from multiprocessing.dummy import Pool\n",
        "\n",
        "from datetime import datetime as dati\n",
        "from pytz import timezone\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def file_size(i):\n",
        "  n, p, d = 0, 1024, ['Bytes', 'KB', 'MB', 'GB', 'TB']\n",
        "  while i > p:\n",
        "    i /= p\n",
        "    n += 1\n",
        "  return '{:.2f} {}'.format(i, d[n])\n",
        "\n",
        "def dati_now():\n",
        "  return dati.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "def ffmpeg_get_screenshot(i):\n",
        "  with open(os.devnull, 'wb') as devnull:\n",
        "    subprocess.check_call(['ffmpeg', '-ss', i[0], '-i', i[1], '-y', '-vframes', '1', i[2]], stdout=devnull, stderr=subprocess.STDOUT)\n",
        "\n",
        "def start_collage(list_of_images, label_list, video_thumb_folder, video_name, vi_de):\n",
        "  \n",
        "  n = len(list_of_images)\n",
        "  number_of_columns = 7\n",
        "  number_of_rows    = 7\n",
        "  matrix_total_size = number_of_rows * number_of_columns\n",
        "  number_of_collages = n / (matrix_total_size)\n",
        "  number_of_collages = int(number_of_collages)+1 if number_of_collages > int(number_of_collages) else int(number_of_collages)\n",
        "  collage_img_type = 'JPEG'\n",
        "\n",
        "  black_webp = 'Temp_Blank.{}'.format(collage_img_type)\n",
        "  bl_img = Image.new('RGB', (1000, 1000))\n",
        "  bl_img.save(black_webp, collage_img_type)\n",
        "\n",
        "  blank_img_to_add = (matrix_total_size*number_of_collages) - n\n",
        "  for i in range(blank_img_to_add):\n",
        "    list_of_images.append(black_webp)\n",
        "    label_list.append('')\n",
        "  \n",
        "  for i in range(number_of_collages):\n",
        "    output_collage = \"{}/C_{} - {}.{}\".format(video_thumb_folder, video_name.replace('.', '_'), i+1, collage_img_type.lower())\n",
        "    create_collage_image(list_of_images[matrix_total_size*i:matrix_total_size*(i+1)], output_collage, vi_de, img_type=collage_img_type, width=2000, height=2000, cols=number_of_columns, rows=number_of_rows, label_list=label_list[matrix_total_size*i:matrix_total_size*(i+1)], frame_index=i)\n",
        "  os.remove(black_webp)\n",
        "\n",
        "def create_collage_image(listofimages, o_file, vi_de, img_type='WEBP', width=2000, height=2000,cols=5,rows=4, label_list=[], frame_index=1):\n",
        "  \n",
        "  if cols == 0:\n",
        "    return\n",
        "  new_im = Image.new('RGB', (width, height), (20, 20, 20))\n",
        "  draw = ImageDraw.Draw(new_im)\n",
        "  font = ImageFont.truetype('Aerial.ttf', 20)\n",
        "  \n",
        "  # Details Pane\n",
        "  details_pane_percentage = 0.1\n",
        "  details_pane_height_end = int(height * details_pane_percentage)\n",
        "  \n",
        "  de_file_name  = vi_de['format']['filename']\n",
        "  de_file_size  = file_size(int(vi_de['format']['size']))\n",
        "  de_duration   = time.strftime('%H H: %M M: %S S', time.gmtime(float(vi_de['format']['duration'])))\n",
        "  de_framerate  = vi_de['streams'][0]['r_frame_rate'].split('/')\n",
        "  de_framerate  = '{:.2f} frames/sec'.format(int(de_framerate[0])/int(de_framerate[1]))\n",
        "  de_video_type = '{} ({})'.format(vi_de['format']['format_long_name'], vi_de['streams'][0]['codec_long_name'])\n",
        "  de_audio_type = '{} ({}), {}Hz'.format(vi_de['streams'][1]['codec_name'], vi_de['streams'][1]['channel_layout'], vi_de['streams'][1]['sample_rate'])\n",
        "  \n",
        "  de_subtitle   = \"No Subtitle\"\n",
        "  for i in vi_de['streams']:\n",
        "    if i[\"codec_type\"] == \"subtitle\":\n",
        "      de_subtitle   = i[\"codec_long_name\"]\n",
        "      break\n",
        "  \n",
        "  details = [\n",
        "              \"Video Name : {} ({})\".format(de_file_name, de_file_size),\n",
        "              \"Timestamp   : {}, Collage-{}\".format(dati_now(), frame_index),\n",
        "              \"Duration       : {}\".format(de_duration),\n",
        "              \"Resolution    : {}x{} / {}\".format(vi_de['streams'][0]['width'], vi_de['streams'][0]['height'], de_framerate),\n",
        "              \"Video Type   : {}\".format(de_video_type),\n",
        "              \"Audio Type   : {}\".format(de_audio_type),\n",
        "              \"Subtitles      : {}\".format(de_subtitle)\n",
        "            ]\n",
        "  \n",
        "  details_pane = Image.new('RGB', (width, details_pane_height_end), (70, 70, 70))\n",
        "  details_draw = ImageDraw.Draw(details_pane)\n",
        "  \n",
        "  details_line_height = int(details_pane_height_end/(len(details)+2))\n",
        "  x = int(width*0.07)\n",
        "  y = details_line_height\n",
        "  \n",
        "  for i in details:\n",
        "    details_draw.text((x, y), i,(255,255,255),font=font)\n",
        "    y += details_line_height\n",
        "  new_im.paste(details_pane, (0, 0))\n",
        "  \n",
        "  # Pictures Pane\n",
        "  collage_width  = int(width  * (0.9))\n",
        "  collage_height = int(height * (1-details_pane_percentage))\n",
        "  thumbnail_width = collage_width//cols\n",
        "  thumbnail_height = collage_height//rows\n",
        "  size = thumbnail_width, thumbnail_height\n",
        "  ims = []\n",
        "  text_coord = []\n",
        "  \n",
        "  for p in listofimages:\n",
        "    try:\n",
        "      im = Image.open(p)\n",
        "    except:\n",
        "      im = Image.new('RGB', (500,500))\n",
        "    im.thumbnail(size)\n",
        "    ims.append(im)\n",
        "    \n",
        "  i = 0\n",
        "  x = int(thumbnail_width*0.1)\n",
        "  y = int(details_pane_height_end * 1.1)\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      text_coord.append([x + thumbnail_width*.2, y + thumbnail_height*.55])\n",
        "      new_im.paste(ims[i], (x, y))\n",
        "      i += 1\n",
        "      x += int(thumbnail_width * 1.1)\n",
        "    y += int(thumbnail_height * 0.9)\n",
        "    x = int(thumbnail_width*0.1)\n",
        "    \n",
        "  for index_i, i in enumerate(text_coord):\n",
        "    draw.text((i[0], i[1]), label_list[index_i],(255,255,255),font=font)\n",
        "\n",
        "  new_im.save(o_file,img_type)\n",
        "\n",
        "def video_to_img(video_path, video_thumb_folder, for_seconds=60):\n",
        "  \n",
        "  video_thumb_folder = \"{}/{} - {}_Thumbs\".format(video_thumb_folder, dati_now(), os.path.splitext(os.path.split(video_path)[1])[0])\n",
        "  video_name = os.path.split(video_path)[1]\n",
        "  \n",
        "  if not os.path.exists(video_thumb_folder):\n",
        "    os.makedirs(video_thumb_folder)\n",
        "    \n",
        "  if not os.path.exists(video_path):\n",
        "    print('File does not Exists :', video_path)\n",
        "    return\n",
        "  \n",
        "  vi_de = ffmpeg.probe(video_path)    \n",
        "  video_duration = float(vi_de['format']['duration'])\n",
        "  no_of_imgs = int(video_duration/for_seconds)\n",
        "  \n",
        "  print('{:>5s} Seconds - {:>6s} Parts - Path : {}\\n'.format(str(int(video_duration)), str(no_of_imgs), video_path))\n",
        "  zeros_size = len(str(no_of_imgs))\n",
        "  list_of_images = []\n",
        "  label_list = []\n",
        "  ffmpeg_commands = []\n",
        "  \n",
        "  time_pointer_count = 1\n",
        "  for i in range(1, no_of_imgs+1 ):\n",
        "    time_pointer =  time.strftime('%H:%M:%S', time.gmtime(i*for_seconds))\n",
        "    label_list.append('({}) {}'.format(time_pointer_count, time_pointer))\n",
        "    time_pointer_count += 1\n",
        "    \n",
        "    out_img_path = \"{}/F_{}.jpg\".format(video_thumb_folder, time_pointer.replace(':', '_'))\n",
        "    list_of_images.append(out_img_path)\n",
        "    ffmpeg_commands.append([time_pointer, video_path, out_img_path])\n",
        "  \n",
        "  with Pool(10) as p:\n",
        "    r = list(tqdm(p.imap(ffmpeg_get_screenshot, ffmpeg_commands), total=len(ffmpeg_commands)))\n",
        "  \n",
        "  start_collage(list_of_images, label_list, video_thumb_folder, video_name, vi_de)\n",
        "  print('\\nPhoto Collage Created')\n",
        "  \n",
        "  if os.name == 'nt':\n",
        "    os.system('start %windir%\\explorer.exe \"{}\"'.format(os.path.abspath(video_thumb_folder)))\n",
        "print('\\r       Ready      ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZIhb_O7DDxG"
      },
      "source": [
        "video_path         = '/content/1.mp4'\n",
        "video_thumb_folder = '123'\n",
        "for_seconds        = 60\n",
        "\n",
        "video_to_img(video_path, video_thumb_folder, for_seconds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrrJPeTk7L7D"
      },
      "source": [
        "## Youtube Downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP8VdVqQ_YQK"
      },
      "source": [
        "!pip install youtube-dl\n",
        "import youtube_dl\n",
        "import json\n",
        "import os\n",
        "# ydl = youtube_dl.YoutubeDL({'outtmpl': '%(id)s.%(ext)s'})\n",
        "# ydl = youtube_dl.YoutubeDL({'outtmpl': '%(title)s-%(id)s.%(ext)s'})\n",
        "\n",
        "def thumbnail_downloader(url, img_name):\n",
        "  !wget $url -O\"$img_name\" >/dev/null 2>&1\n",
        "  # from IPython.display import Image\n",
        "  # Image('1.jpg')\n",
        "\n",
        "def save_json(result, file_name):\n",
        "  video_detail = {}\n",
        "  video_detail['Id']          = result['id']\n",
        "  video_detail['Title']       = result['title']\n",
        "  video_detail['Duration']    = result['duration']\n",
        "  video_detail['Upload_date'] = result['upload_date']\n",
        "  video_detail['Description'] = result['description']\n",
        "\n",
        "  video_detail['Uploader']    = result['uploader']\n",
        "  video_detail['Channel_url'] = result['channel_url']\n",
        "\n",
        "  video_detail['Thumbnail']   = result['thumbnail']\n",
        "  video_detail['Categories']  = result['categories']\n",
        "  video_detail['Tags']        = result['tags']\n",
        "\n",
        "  video_detail['View_count']  = result['view_count']\n",
        "  video_detail['Like_count']  = result['like_count']\n",
        "  video_detail['Dislike_count'] = result['dislike_count']\n",
        "  video_detail['No_formats']  = len(result['formats'])\n",
        "  video_detail['Z_Technical_details']     = result['formats'][-1]\n",
        "\n",
        "  with open(file_name, 'w')as f1:\n",
        "    f1.write(json.dumps(video_detail, indent=4, sort_keys=True))\n",
        "  # print(json.dumps(video_detail, indent=4, sort_keys=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8nXY9hJG91m"
      },
      "source": [
        "def download_video(url, folder):\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "  ydl = youtube_dl.YoutubeDL({'outtmpl': '{}/%(title)s-%(id)s.%(ext)s'.format(folder)})\n",
        "  with ydl:\n",
        "    result = ydl.extract_info(url)\n",
        "  \n",
        "  if video_name == '':\n",
        "    pass\n",
        "  json_name  = video_name.replace('.mp4', '.json')\n",
        "  thumb_name = video_name.replace('.mp4', '.jpg')\n",
        "\n",
        "  thumbnail_downloader(result['thumbnails'][-1]['url'], thumb_name)\n",
        "  save_json(result, json_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJY-MkS8HN7C"
      },
      "source": [
        "download_video(url, folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjWLjmFW9WWw"
      },
      "source": [
        "video_url = 'https://www.youtube.com/watch?v=WsBApPjUG4k'\n",
        "\n",
        "with ydl:\n",
        "  result = ydl.extract_info(video_url, download=False)\n",
        "\n",
        "if 'entries' in result:\n",
        "  # Can be a playlist or a list of videos\n",
        "  video = result['entries'][0]\n",
        "else:\n",
        "  # Just a video\n",
        "  video = result\n",
        "\n",
        "print(video)\n",
        "# video_url = video['url']\n",
        "# print(video_url)\n",
        "with open('1.json', 'w')as f1:\n",
        "  f1.write(json.dumps(video, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jITdGdZfI2O"
      },
      "source": [
        "with open('1.json', 'w')as f1:\n",
        "  f1.write(json.dumps(result, indent=4, sort_keys=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}